{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/data')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/utils')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/vision')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv_utils\n",
    "# import file_utils as futils\n",
    "# import data_loader as dload\n",
    "# import vision\n",
    "from vision.engine import train_one_epoch, evaluate\n",
    "from vision.v_utils import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = [\"Drone\", \"Background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcc47d8d9d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDroneTorchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a Drone Yolo dataset.\n",
    "    \n",
    "    Args:\n",
    "        split: train, test or val\n",
    "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path='.',\n",
    "        split='test',\n",
    "        transforms=None,\n",
    "        classes=classes_list,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.classes = classes\n",
    "\n",
    "        self.images_path = []\n",
    "        self.labels_path = []\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        images_folder = os.path.join(dataset_path, 'images',split)\n",
    "        labels_folder = os.path.join(dataset_path, 'labels', split)\n",
    "        for image in os.listdir(images_folder):\n",
    "          img_path = os.path.join(images_folder, image)\n",
    "        \n",
    "          label = image.split('.')[0] + '.txt'\n",
    "          label_path = os.path.join(labels_folder, label)\n",
    "          with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines()\n",
    "          if len(label_lines) != 0:   \n",
    "            self.labels_path.append(label_path)\n",
    "            self.images_path.append(img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # reading the images and converting them to correct color  \n",
    "        img_path = self.images_path[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # # prepairing target\n",
    "        label_path = self.labels_path[idx] \n",
    "        with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines() \n",
    "        \n",
    "        # cv2 image gives size as height x width    \n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # detections = sample[self.gt_field].detections\n",
    "        for label in label_lines:\n",
    "            class_id, x_center, y_center, bbox_width, bbox_height = map(float, label.split())\n",
    "            if x_center > 1 or y_center > 1 or  bbox_width > 1 or  bbox_height > 1:\n",
    "                print(\"label_path:\",label_path)\n",
    "            boxes.append([x_center * wt, y_center * ht, (x_center + bbox_width) * wt, (x_center + bbox_height) * ht])\n",
    "            labels.append(1)  # drone class\n",
    "\n",
    "        \n",
    "        # applying augmentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n",
    "            img = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"category_ids\"]\n",
    "\n",
    "        # convert boxes into a torch.Tensor                \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)                \n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.as_tensor([idx])\n",
    "\n",
    "        if len(boxes) != 0:\n",
    "            # getting the areas of the boxes\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        target[\"iscrowd\"] = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.3 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(320),\n",
    "        # A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n",
    "        A.RandomSizedBBoxSafeCrop(width=300, height=300, erosion_rate=0.1),\n",
    "     \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n",
    "        A.ToFloat(max_value=255, p=1, always_apply=True),\n",
    "\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']),\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(300),\n",
    "        # A.PadIfNeeded(min_height=300, min_width=300, border_mode=0),\n",
    "        A.ToFloat(max_value=255, p=1, always_apply=True),\n",
    "\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_train_dataset = YoloDroneTorchDataset('../dataset/yolo', 'train', test_transform) \n",
    "yolo_test_dataset = YoloDroneTorchDataset('../dataset/yolo', 'test', test_transform)\n",
    "yolo_val_dataset = YoloDroneTorchDataset('../dataset/yolo', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630 2706 2708\n"
     ]
    }
   ],
   "source": [
    "print(len(yolo_train_dataset), len(yolo_test_dataset), len(yolo_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.ssd import SSDHead\n",
    "\n",
    "head = SSDHead(in_channels=[512, 1024, 512, 256, 256, 256] , num_anchors=[4,6,6,6,4,4] , num_classes=2)\n",
    "model.head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "test_bs = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    yolo_train_dataset, batch_size=bs, shuffle=True, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    yolo_test_dataset, batch_size=test_bs, shuffle=False, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using device %s\" % device)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15013/559415089.py:80: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_15013/559415089.py\", line 70, in __getitem__\n    transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 346, in __call__\n    self.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 380, in preprocess\n    p.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 160, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 174, in check_and_convert\n    return process_func(data, image_shape)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 155, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, image_shape, check_validity=True)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 294, in convert_bboxes_to_albumentations\n    check_bboxes(converted_bboxes)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 371, in check_bboxes\n    raise ValueError(\nValueError: Expected x_max for bbox [0.77708334 0.27083334 1.6083333  1.08125    1.        ] to be in the range [0.0, 1.0], got 1.6083333492279053.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/hse/hse_dl_project/src/vision/engine.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     21\u001b[0m     warmup_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_loader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLinearLR(\n\u001b[1;32m     24\u001b[0m         optimizer, start_factor\u001b[38;5;241m=\u001b[39mwarmup_factor, total_iters\u001b[38;5;241m=\u001b[39mwarmup_iters\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m metric_logger\u001b[38;5;241m.\u001b[39mlog_every(data_loader, print_freq, header):\n\u001b[1;32m     28\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     29\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[0;32m~/hse/hse_dl_project/src/vision/v_utils.py:170\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    166\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    167\u001b[0m         [header, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime: \u001b[39m\u001b[38;5;132;01m{time}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    171\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_15013/559415089.py\", line 70, in __getitem__\n    transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 346, in __call__\n    self.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 380, in preprocess\n    p.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 160, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 174, in check_and_convert\n    return process_func(data, image_shape)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 155, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, image_shape, check_validity=True)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 294, in convert_bboxes_to_albumentations\n    check_bboxes(converted_bboxes)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 371, in check_bboxes\n    raise ValueError(\nValueError: Expected x_max for bbox [0.77708334 0.27083334 1.6083333  1.08125    1.        ] to be in the range [0.0, 1.0], got 1.6083333492279053.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
