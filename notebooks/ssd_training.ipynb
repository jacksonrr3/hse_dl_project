{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "import pybboxes as pbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/data')))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/utils')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src/vision')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv_utils\n",
    "# import file_utils as futils\n",
    "# import data_loader as dload\n",
    "# import vision\n",
    "from vision.engine import train_one_epoch, evaluate\n",
    "from vision.v_utils import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = [\"Drone\", \"Background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f551be579f0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloDroneTorchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a Drone Yolo dataset.\n",
    "    \n",
    "    Args:\n",
    "        split: train, test or val\n",
    "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path='.',\n",
    "        split='test',\n",
    "        transforms=None,\n",
    "        classes=classes_list,\n",
    "    ):\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.classes = classes\n",
    "\n",
    "        self.images_path = []\n",
    "        self.labels_path = []\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        images_folder = os.path.join(dataset_path, 'images',split)\n",
    "        labels_folder = os.path.join(dataset_path, 'labels', split)\n",
    "        for image in os.listdir(images_folder):\n",
    "          img_path = os.path.join(images_folder, image)\n",
    "        \n",
    "          label = image.split('.')[0] + '.txt'\n",
    "          label_path = os.path.join(labels_folder, label)\n",
    "          with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines()\n",
    "          if len(label_lines) != 0:   \n",
    "            self.labels_path.append(label_path)\n",
    "            self.images_path.append(img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # reading the images and converting them to correct color  \n",
    "        img_path = self.images_path[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # # prepairing target\n",
    "        label_path = self.labels_path[idx] \n",
    "        with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines() \n",
    "        \n",
    "        # cv2 image gives size as height x width    \n",
    "        wt = img.shape[1]\n",
    "        ht = img.shape[0]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # detections = sample[self.gt_field].detections\n",
    "        for label in label_lines:\n",
    "            class_id, x_center, y_center, bbox_width, bbox_height = map(float, label.split())\n",
    "            yolo_bbox = (x_center, y_center, bbox_width, bbox_height)\n",
    "            voc_bbox = pbx.convert_bbox(yolo_bbox, from_type=\"yolo\", to_type=\"voc\", image_size=(wt, ht))\n",
    "\n",
    "            boxes.append([voc_bbox[0], voc_bbox[1], voc_bbox[2], voc_bbox[3]])\n",
    "            # boxes.append([x_center * wt, y_center * ht, (x_center + bbox_width) * wt, (x_center + bbox_height) * ht])\n",
    "            labels.append(1)  # drone class\n",
    "\n",
    "        \n",
    "        # applying augmentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n",
    "            img = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"category_ids\"]\n",
    "\n",
    "        # convert boxes into a torch.Tensor                \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)                \n",
    "            \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.as_tensor([idx])\n",
    "\n",
    "        if len(boxes) != 0:\n",
    "            # getting the areas of the boxes\n",
    "            target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        # suppose all instances are not crowd\n",
    "        target[\"iscrowd\"] = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_format = 'pascal_voc'\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(320),\n",
    "        # A.PadIfNeeded(min_height=320, min_width=320, border_mode=0),\n",
    "        A.RandomSizedBBoxSafeCrop(width=300, height=300, erosion_rate=0.1),\n",
    "     \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.3),\n",
    "        A.ToFloat(max_value=255, p=1, always_apply=True),\n",
    "\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids']),\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(300),\n",
    "        # A.PadIfNeeded(min_height=300, min_width=300, border_mode=0),\n",
    "        A.ToFloat(max_value=255, p=1, always_apply=True),\n",
    "\n",
    "        ToTensorV2(p=1.0)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_train_dataset = YoloDroneTorchDataset('../dataset/yolo', 'train', test_transform) \n",
    "yolo_test_dataset = YoloDroneTorchDataset('../dataset/yolo', 'test', test_transform)\n",
    "yolo_val_dataset = YoloDroneTorchDataset('../dataset/yolo', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630 2706 2708\n"
     ]
    }
   ],
   "source": [
    "print(len(yolo_train_dataset), len(yolo_test_dataset), len(yolo_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.detection.ssd300_vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.ssd import SSDHead\n",
    "\n",
    "head = SSDHead(in_channels=[512, 1024, 512, 256, 256, 256] , num_anchors=[4,6,6,6,4,4] , num_classes=2)\n",
    "model.head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2\n",
    "test_bs = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    yolo_train_dataset, batch_size=bs, shuffle=True, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    yolo_test_dataset, batch_size=test_bs, shuffle=False, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Using device %s\" % device)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=learning_rate,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1073/955831022.py:82: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
      "/tmp/ipykernel_1073/955831022.py:82: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/6315]  eta: 2:22:24  lr: 0.000000  loss: 3.8915 (3.8915)  bbox_regression: 1.5395 (1.5395)  classification: 2.3520 (2.3520)  time: 1.3531  data: 0.1175  max mem: 1412\n",
      "Epoch: [0]  [  10/6315]  eta: 1:21:37  lr: 0.000000  loss: 3.7060 (3.6307)  bbox_regression: 1.4655 (1.3730)  classification: 2.2405 (2.2577)  time: 0.7767  data: 0.0195  max mem: 1412\n",
      "Epoch: [0]  [  20/6315]  eta: 1:18:20  lr: 0.000000  loss: 3.5191 (3.6557)  bbox_regression: 1.2150 (1.3795)  classification: 2.2391 (2.2762)  time: 0.7163  data: 0.0087  max mem: 1412\n",
      "Epoch: [0]  [  30/6315]  eta: 1:17:05  lr: 0.000000  loss: 3.6292 (3.9853)  bbox_regression: 1.3169 (1.6614)  classification: 2.2849 (2.3239)  time: 0.7135  data: 0.0080  max mem: 1412\n",
      "Epoch: [0]  [  40/6315]  eta: 1:16:23  lr: 0.000001  loss: 3.5922 (3.9404)  bbox_regression: 1.3169 (1.6294)  classification: 2.2705 (2.3110)  time: 0.7134  data: 0.0079  max mem: 1412\n",
      "Epoch: [0]  [  50/6315]  eta: 1:15:56  lr: 0.000001  loss: 3.1169 (3.7961)  bbox_regression: 0.8419 (1.4928)  classification: 2.2550 (2.3033)  time: 0.7138  data: 0.0074  max mem: 1412\n",
      "Epoch: [0]  [  60/6315]  eta: 1:15:37  lr: 0.000001  loss: 3.1534 (3.7112)  bbox_regression: 0.8419 (1.4065)  classification: 2.2750 (2.3047)  time: 0.7150  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [  70/6315]  eta: 1:15:20  lr: 0.000001  loss: 3.3855 (3.7538)  bbox_regression: 1.0855 (1.4508)  classification: 2.2915 (2.3030)  time: 0.7150  data: 0.0078  max mem: 1412\n",
      "Epoch: [0]  [  80/6315]  eta: 1:15:05  lr: 0.000001  loss: 3.6752 (3.7195)  bbox_regression: 1.3987 (1.4214)  classification: 2.2607 (2.2981)  time: 0.7140  data: 0.0074  max mem: 1412\n",
      "Epoch: [0]  [  90/6315]  eta: 1:14:51  lr: 0.000001  loss: 3.6671 (3.7546)  bbox_regression: 1.4393 (1.4536)  classification: 2.2528 (2.3009)  time: 0.7133  data: 0.0072  max mem: 1412\n",
      "Epoch: [0]  [ 100/6315]  eta: 1:14:40  lr: 0.000001  loss: 3.7477 (3.7740)  bbox_regression: 1.4949 (1.4716)  classification: 2.2709 (2.3024)  time: 0.7139  data: 0.0072  max mem: 1412\n",
      "Epoch: [0]  [ 110/6315]  eta: 1:14:29  lr: 0.000001  loss: 3.3564 (3.7721)  bbox_regression: 1.1299 (1.4691)  classification: 2.2598 (2.3029)  time: 0.7146  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [ 120/6315]  eta: 1:14:19  lr: 0.000002  loss: 3.4208 (3.8104)  bbox_regression: 1.1564 (1.5071)  classification: 2.2583 (2.3032)  time: 0.7145  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [ 130/6315]  eta: 1:14:09  lr: 0.000002  loss: 3.3612 (3.7919)  bbox_regression: 1.0536 (1.4896)  classification: 2.2773 (2.3023)  time: 0.7143  data: 0.0073  max mem: 1412\n",
      "Epoch: [0]  [ 140/6315]  eta: 1:13:59  lr: 0.000002  loss: 3.3923 (3.8134)  bbox_regression: 1.0522 (1.5098)  classification: 2.2551 (2.3036)  time: 0.7139  data: 0.0073  max mem: 1412\n",
      "Epoch: [0]  [ 150/6315]  eta: 1:13:50  lr: 0.000002  loss: 3.6326 (3.8084)  bbox_regression: 1.3361 (1.5072)  classification: 2.2551 (2.3013)  time: 0.7142  data: 0.0072  max mem: 1412\n",
      "Epoch: [0]  [ 160/6315]  eta: 1:13:40  lr: 0.000002  loss: 3.4254 (3.8215)  bbox_regression: 1.2355 (1.5209)  classification: 2.2688 (2.3006)  time: 0.7132  data: 0.0071  max mem: 1412\n",
      "Epoch: [0]  [ 170/6315]  eta: 1:13:31  lr: 0.000002  loss: 3.4254 (3.8245)  bbox_regression: 1.1566 (1.5242)  classification: 2.2609 (2.3003)  time: 0.7120  data: 0.0070  max mem: 1412\n",
      "Epoch: [0]  [ 180/6315]  eta: 1:13:22  lr: 0.000002  loss: 3.6219 (3.8622)  bbox_regression: 1.2926 (1.5608)  classification: 2.2609 (2.3014)  time: 0.7127  data: 0.0075  max mem: 1412\n",
      "Epoch: [0]  [ 190/6315]  eta: 1:13:14  lr: 0.000003  loss: 3.7544 (3.8630)  bbox_regression: 1.4675 (1.5605)  classification: 2.3056 (2.3024)  time: 0.7136  data: 0.0079  max mem: 1412\n",
      "Epoch: [0]  [ 200/6315]  eta: 1:13:06  lr: 0.000003  loss: 3.6247 (3.8544)  bbox_regression: 1.3663 (1.5504)  classification: 2.2802 (2.3040)  time: 0.7139  data: 0.0077  max mem: 1412\n",
      "Epoch: [0]  [ 210/6315]  eta: 1:12:57  lr: 0.000003  loss: 3.6037 (3.8681)  bbox_regression: 1.4866 (1.5650)  classification: 2.2668 (2.3031)  time: 0.7134  data: 0.0080  max mem: 1412\n",
      "Epoch: [0]  [ 220/6315]  eta: 1:12:49  lr: 0.000003  loss: 3.4392 (3.8677)  bbox_regression: 1.1645 (1.5610)  classification: 2.2677 (2.3067)  time: 0.7133  data: 0.0083  max mem: 1412\n",
      "Epoch: [0]  [ 230/6315]  eta: 1:12:41  lr: 0.000003  loss: 3.4401 (3.8835)  bbox_regression: 1.1669 (1.5771)  classification: 2.2733 (2.3064)  time: 0.7139  data: 0.0079  max mem: 1412\n",
      "Epoch: [0]  [ 240/6315]  eta: 1:12:33  lr: 0.000003  loss: 3.5326 (3.8794)  bbox_regression: 1.2370 (1.5754)  classification: 2.2495 (2.3040)  time: 0.7138  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [ 250/6315]  eta: 1:12:25  lr: 0.000003  loss: 3.7124 (3.8903)  bbox_regression: 1.5020 (1.5852)  classification: 2.2736 (2.3052)  time: 0.7136  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [ 260/6315]  eta: 1:12:17  lr: 0.000003  loss: 3.8068 (3.8869)  bbox_regression: 1.4803 (1.5786)  classification: 2.3008 (2.3083)  time: 0.7131  data: 0.0076  max mem: 1412\n",
      "Epoch: [0]  [ 270/6315]  eta: 1:12:09  lr: 0.000004  loss: 3.1864 (3.8704)  bbox_regression: 0.9327 (1.5634)  classification: 2.2814 (2.3070)  time: 0.7130  data: 0.0075  max mem: 1412\n",
      "Epoch: [0]  [ 280/6315]  eta: 1:12:02  lr: 0.000004  loss: 3.0880 (3.8443)  bbox_regression: 0.8293 (1.5384)  classification: 2.2614 (2.3059)  time: 0.7136  data: 0.0073  max mem: 1412\n",
      "Epoch: [0]  [ 290/6315]  eta: 1:11:54  lr: 0.000004  loss: 3.1515 (3.8353)  bbox_regression: 0.8454 (1.5306)  classification: 2.2565 (2.3047)  time: 0.7141  data: 0.0073  max mem: 1412\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1073/955831022.py\", line 72, in __getitem__\n    transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 346, in __call__\n    self.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 380, in preprocess\n    p.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 160, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 174, in check_and_convert\n    return process_func(data, image_shape)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 155, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, image_shape, check_validity=True)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 294, in convert_bboxes_to_albumentations\n    check_bboxes(converted_bboxes)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 371, in check_bboxes\n    raise ValueError(\nValueError: Expected y_min for bbox [ 0.8979167  -0.02708333  0.9625      0.025       1.        ] to be in the range [0.0, 1.0], got -0.02708333358168602.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/hse/hse_dl_project/src/vision/engine.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq, scaler)\u001b[0m\n\u001b[1;32m     21\u001b[0m     warmup_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data_loader) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLinearLR(\n\u001b[1;32m     24\u001b[0m         optimizer, start_factor\u001b[38;5;241m=\u001b[39mwarmup_factor, total_iters\u001b[38;5;241m=\u001b[39mwarmup_iters\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m metric_logger\u001b[38;5;241m.\u001b[39mlog_every(data_loader, print_freq, header):\n\u001b[1;32m     28\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[1;32m     29\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n",
      "File \u001b[0;32m~/hse/hse_dl_project/src/vision/v_utils.py:170\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    166\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    167\u001b[0m         [header, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m space_fmt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta: \u001b[39m\u001b[38;5;132;01m{eta}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{meters}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime: \u001b[39m\u001b[38;5;132;01m{time}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata: \u001b[39m\u001b[38;5;132;01m{data}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m MB \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024.0\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024.0\u001b[39m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    171\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1073/955831022.py\", line 72, in __getitem__\n    transformed = self.transforms(image=img,bboxes=boxes, category_ids=labels)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 346, in __call__\n    self.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/composition.py\", line 380, in preprocess\n    p.preprocess(data)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 160, in preprocess\n    data[data_name] = self.check_and_convert(data[data_name], image_shape, direction=\"to\")\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/utils.py\", line 174, in check_and_convert\n    return process_func(data, image_shape)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 155, in convert_to_albumentations\n    return convert_bboxes_to_albumentations(data, self.params.format, image_shape, check_validity=True)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 294, in convert_bboxes_to_albumentations\n    check_bboxes(converted_bboxes)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/augmentations/utils.py\", line 180, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/jacksonrr3/hse/hse_dl_project/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py\", line 371, in check_bboxes\n    raise ValueError(\nValueError: Expected y_min for bbox [ 0.8979167  -0.02708333  0.9625      0.025       1.        ] to be in the range [0.0, 1.0], got -0.02708333358168602.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=10)\n",
    "\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/ssd/ssd300_drone_10epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for s in ['train', 'test', 'val']:\n",
    "    labels_folder = os.path.join('../dataset/yolo/labels/',  s)\n",
    "    for label_file in os.listdir(labels_folder):\n",
    "        label_path = os.path.join(labels_folder, label_file)\n",
    "        with open(label_path, 'r') as f:\n",
    "            label_lines = f.readlines() \n",
    "\n",
    "        for label in label_lines:\n",
    "            class_id, x_center, y_center, bbox_width, bbox_height = map(float, label.split())\n",
    "            yolo_bbox = (x_center, y_center, bbox_width, bbox_height)\n",
    "            voc_bbox = pbx.convert_bbox(yolo_bbox, from_type=\"yolo\", to_type=\"voc\", image_size=(480, 480))\n",
    "            for b in voc_bbox:\n",
    "                if b > 0 and b < 1:\n",
    "                    print(label_path)\n",
    "                    print(voc_bbox)\n",
    "                    counter += 1        \n",
    "\n",
    "\n",
    "\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
